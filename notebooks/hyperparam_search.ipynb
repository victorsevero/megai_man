{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "import optuna\n",
    "from optuna.samplers import RandomSampler\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from megai_man.env import make_venv\n",
    "from megai_man.callbacks import (\n",
    "    StageLoggingCallback,\n",
    "    TrainingStatsLoggerCallback,\n",
    "    StopTrainingOnTimeBudget,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-30 16:35:31,437] Using an existing study with name 'cutman_random_searcher' instead of creating a new one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d8b8c206454cf6be90e5c78fa777f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sample_params(trial: optuna.Trial):\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [128, 256, 512, 1024, 2048])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-2, log=True)\n",
    "    clip_range = trial.suggest_float(\"clip_range\", 0.1, 0.3, step=0.1)\n",
    "    vf_coef = trial.suggest_float(\"vf_coef\", 0.5, 1.0)\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 1e-5, 1e-1, log=True)\n",
    "    gae_lambda = trial.suggest_float(\"gae_lambda\", 0.9, 1.0, log=True)\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 4, 10)\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.99, 0.995, 0.999])\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.5, 1)\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"clip_range\": clip_range,\n",
    "        \"vf_coef\": vf_coef,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"gamma\": gamma,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "    }\n",
    "\n",
    "\n",
    "def optimizer(tensorboard_log: str, sample_fn, timesteps_per_trial=1_000_000):\n",
    "    def optimize_agent(trial):\n",
    "        env_kwargs = {\n",
    "            \"n_envs\": 8,\n",
    "            \"state\": \"CutMan\",\n",
    "            \"screen\": None,\n",
    "            \"frameskip\": 4,\n",
    "            \"frame_stack\": 4,\n",
    "            \"truncate_if_no_improvement\": True,\n",
    "            \"obs_space\": \"screen\",\n",
    "            \"action_space\": \"multi_discrete\",\n",
    "            \"crop_img\": False,\n",
    "            \"invincible\": False,\n",
    "            \"no_enemies\": False,\n",
    "            \"render_mode\": None,\n",
    "            \"damage_terminate\": False,\n",
    "            \"fixed_damage_punishment\": 0.05,\n",
    "            \"forward_factor\": 0.05,\n",
    "            \"backward_factor\": 0.055,\n",
    "            \"time_punishment_factor\": 0,\n",
    "            \"multi_input\": True,\n",
    "            \"curriculum\": False,\n",
    "            \"screen_rewards\": False,\n",
    "            \"score_reward\": 0,\n",
    "            \"distance_only_on_ground\": True,\n",
    "            \"term_back_screen\": True,\n",
    "            \"_enforce_subproc\": False,\n",
    "        }\n",
    "\n",
    "        venv = make_venv(**env_kwargs)\n",
    "\n",
    "        model_params = sample_fn(trial)\n",
    "        model = PPO(\n",
    "            policy=\"MultiInputPolicy\",\n",
    "            env=venv,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            verbose=0,\n",
    "            seed=666,\n",
    "            device=\"cuda\",\n",
    "            **model_params,\n",
    "        )\n",
    "        model.learn(\n",
    "            timesteps_per_trial,\n",
    "            callback=[\n",
    "                StageLoggingCallback(),\n",
    "                TrainingStatsLoggerCallback(),\n",
    "                StopTrainingOnTimeBudget(budget=60 * 60),  # 1 hour\n",
    "            ],\n",
    "            log_interval=1,\n",
    "        )\n",
    "        venv.close()\n",
    "\n",
    "        eval_venv = make_venv(**{**env_kwargs, \"n_envs\": 1})\n",
    "        reward, _ = evaluate_policy(\n",
    "            model,\n",
    "            eval_venv,\n",
    "            n_eval_episodes=1,\n",
    "            deterministic=True,\n",
    "        )\n",
    "        eval_venv.close()\n",
    "        return reward\n",
    "\n",
    "    return optimize_agent\n",
    "\n",
    "\n",
    "def tune(sample_fn, name, n_trials=500, timesteps_per_trial=1_000_000):\n",
    "    db_path = f\"studies/{name}.db\"\n",
    "    Path(db_path).touch(exist_ok=True)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        storage=f\"sqlite:///{db_path}\",\n",
    "        sampler=RandomSampler(seed=666),\n",
    "        study_name=name,\n",
    "        direction=\"maximize\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "\n",
    "    n_trials -= len([x for x in study.trials if x.state.name == \"COMPLETE\"])\n",
    "\n",
    "    if n_trials:\n",
    "        study.optimize(\n",
    "            optimizer(f\"logs/{name}\", sample_fn, timesteps_per_trial),\n",
    "            n_trials=n_trials,\n",
    "            n_jobs=1,\n",
    "            gc_after_trial=True,\n",
    "            show_progress_bar=True,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Total trials already finished.\")\n",
    "\n",
    "    with open(f\"{name}_opt.yml\", \"w\") as fp:\n",
    "        yaml.safe_dump(study.best_params, fp)\n",
    "\n",
    "\n",
    "tune(\n",
    "    sample_fn=sample_params,\n",
    "    name=\"cutman_random_searcher\",\n",
    "    n_trials=100,\n",
    "    timesteps_per_trial=5_000_000,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megai-man-wnxxQsUz-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
